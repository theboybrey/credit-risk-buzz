{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Final Model Comparison & Evaluation\n",
    "\n",
    "## Section E: Evaluation, Robustness & Scalability\n",
    "\n",
    "This notebook provides comprehensive evaluation including:\n",
    "- Performance metrics comparison across data scales\n",
    "- Robustness analysis (noise sensitivity)\n",
    "- Scalability analysis (throughput, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from config import PROCESSED_DATA_DIR, RESULTS_DIR, RANDOM_STATE\n",
    "from src.evaluation import (\n",
    "    compute_metrics, evaluate_at_scales, evaluate_robustness_noise,\n",
    "    plot_roc_curves, plot_precision_recall_curves,\n",
    "    generate_evaluation_report\n",
    ")\n",
    "from src.visualization import plot_scale_performance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Imports successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "try:\n",
    "    rf_model = joblib.load(RESULTS_DIR / 'randomforest_model.joblib')\n",
    "    xgb_model = joblib.load(RESULTS_DIR / 'xgboost_model.joblib')\n",
    "    print('Models loaded successfully!')\n",
    "except:\n",
    "    print('Models not found. Run notebook 03 first.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv(PROCESSED_DATA_DIR / 'loan_data_processed.csv')\n",
    "\n",
    "X = df.drop(columns=['default', 'loan_status'], errors='ignore')\n",
    "y = df['default']\n",
    "\n",
    "# Encode categorical\n",
    "cat_cols = X.select_dtypes(include=['object', 'string']).columns\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Test set size: {len(X_test):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Selection Justification\n",
    "\n",
    "For **loan default prediction**, we prioritize:\n",
    "\n",
    "1. **ROC-AUC**: Overall discriminative ability across all thresholds\n",
    "2. **Recall (Sensitivity)**: Critical to identify actual defaults (minimize FN)\n",
    "3. **Precision-Recall AUC**: Better metric for imbalanced datasets\n",
    "4. **F1-Score**: Harmonic mean balancing precision and recall\n",
    "\n",
    "**Why not just Accuracy?** With 80% non-defaults, a naive classifier achieves 80% accuracy by predicting all non-defaults. This is useless for risk management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('COMPREHENSIVE MODEL EVALUATION')\n",
    "print('='*60)\n",
    "\n",
    "models = {'RandomForest': rf_model, 'XGBoost': xgb_model}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\n--- {name} ---')\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = compute_metrics(y_test, y_pred, y_proba)\n",
    "    for metric, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f'{metric}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('SCALABILITY ANALYSIS')\n",
    "print('='*60)\n",
    "\n",
    "scales = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\n--- {name} ---')\n",
    "    scale_results = evaluate_at_scales(model, X_test, y_test, scales=scales)\n",
    "    print(scale_results[['scale', 'n_samples', 'roc_auc', 'throughput']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scalability\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for name, model in models.items():\n",
    "    scale_results = evaluate_at_scales(model, X_test, y_test, scales=scales)\n",
    "    \n",
    "    axes[0].plot(scale_results['n_samples'], scale_results['roc_auc'], 'o-', label=name)\n",
    "    axes[1].plot(scale_results['n_samples'], scale_results['throughput'], 'o-', label=name)\n",
    "\n",
    "axes[0].set_xlabel('Number of Samples')\n",
    "axes[0].set_ylabel('ROC-AUC')\n",
    "axes[0].set_title('Performance vs Data Scale')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Number of Samples')\n",
    "axes[1].set_ylabel('Throughput (samples/sec)')\n",
    "axes[1].set_title('Prediction Throughput')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'scalability_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness to Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('ROBUSTNESS TO NOISE')\n",
    "print('='*60)\n",
    "\n",
    "noise_levels = [0, 0.05, 0.1, 0.2, 0.3]\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\n--- {name} ---')\n",
    "    noise_results = evaluate_robustness_noise(model, X_test, y_test, noise_levels=noise_levels)\n",
    "    print(noise_results[['noise_level', 'accuracy', 'f1', 'roc_auc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize robustness\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for name, model in models.items():\n",
    "    noise_results = evaluate_robustness_noise(model, X_test, y_test, noise_levels=noise_levels)\n",
    "    ax.plot(noise_results['noise_level'], noise_results['roc_auc'], 'o-', label=name)\n",
    "\n",
    "ax.set_xlabel('Noise Level (std fraction)')\n",
    "ax.set_ylabel('ROC-AUC')\n",
    "ax.set_title('Model Robustness to Feature Noise')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'robustness_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization Limitations\n",
    "\n",
    "### Identified Limitations:\n",
    "\n",
    "1. **Temporal drift**: Model trained on historical data may not generalize to future economic conditions\n",
    "2. **Geographic bias**: Lending Club data is US-centric; may not apply to other markets\n",
    "3. **Feature availability**: Some features may not be available in real-time scoring\n",
    "4. **Selection bias**: Only approved loans are in the dataset (rejected applications excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print('\\n' + '='*60)\n",
    "print('EVALUATION SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "summary = []\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    metrics = compute_metrics(y_test, y_pred, y_proba)\n",
    "    \n",
    "    summary.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1': metrics['f1'],\n",
    "        'ROC-AUC': metrics['roc_auc'],\n",
    "        'PR-AUC': metrics.get('pr_auc', 0)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "summary_df.to_csv(RESULTS_DIR / 'final_evaluation.csv', index=False)\n",
    "print('\\nResults saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
